---  
title: The Bayesian Way
output: 
  html_document
---
  
## The Bayesian Way
  
### Uncertainty

* in frequentist statistics, the true parameters are fixed, and the data is random
* in Bayesian statistics, the data is given; what's uncertain are the parameters
* in other words, parameters like means or standard deviations are modeled as random variables
* they have a _prior_ (before looking at the data) and a _posterior_ distribution (combining prior and data according to Bayes' theorem)
* the prior information and the data are combined using Bayes Theorem:

$$P(A ~|~ B) = \frac{P(B|A) * P(A)}{P(B)}$$
* the different items being
    + the prior probability of the hypothesis/parameter \(P(A)\)
    + the likelihood of the data, given the hypothesis/parameter \(P(B|A)\)
    + the evidence \(P(B)\) = the likelihood of the data, integrated over all hypotheses/parameter values
    + the posterior probability of the hypothesis/parameter, given the data \(P(A|B)\)


### Bayesian updating

* from Bayes' theorem follows the idea of _Bayesian updating_
* Let's do an example.

We want to estimate the bias of a curious-looking coin we've never seen before, and we're skepticists, so we assume no value is more probable a priori than another. So, we choose a uniform distribution for our prior:

```{r}
library(ggplot2)
library(dplyr)
x <- seq(0,1,0.001)
# using a beta(1,1) distribution for the prior
ggplot(data_frame(x), aes(x)) + stat_function(fun = dbeta, args = list(1,1))
```

That is, before ever seeing any data, we assume that every bias is equally likely.

Now, we throw the coin once and obtain a tail. We now update our prior assumption to take into account the data, using Bayes theorem. 
In our conveniently chosen example, we don't have to compute integrals - we make use of a concept called _conjugacy_, which means that if we choose a prior distribution that is _conjugate_ to the distribution of the likelihood, we obtain a new instance of the type of the prior distribution.
For our example of coin flips, the conjugate distributions are the beta and the binomial, and combining the beta prior with the binomial likelihood of coin flips we obtain a beta distribution again:

```{r}
x <- seq(0,1,0.001)
# having flipped the coin once and seen a tail, we now have a beta(1,12) distribution for the posterior
ggplot(data_frame(x), aes(x)) + stat_function(fun = dbeta, args = list(1,2))
```

As we see from the plot, now that we've seen one tail we can be sure of one thing: The coin cannot have two heads.

We flip again, and obtain another tail.

```{r}
x <- seq(0,1,0.001)
# tail again - posterior is now beta(1,3) 
ggplot(data_frame(x), aes(x)) + stat_function(fun = dbeta, args = list(1,3))
```

Now, we get a head, and can rule out that there's just two tails:

```{r}
x <- seq(0,1,0.001)
# tail again - posterior is now beta(1,3) 
ggplot(data_frame(x), aes(x)) + stat_function(fun = dbeta, args = list(2,3))
```



### Computation and Markov Chain Monte Carlo

* Until recently, using Bayesian models was severely restricted due to computational problems:
    + if not using conjugate distributions, exactly computing the integral in the denominator may be unfeasible
    + on the other hand, a model should be chosen for adequacy, not for computational feasibility
    + solution: Markov Chain Monte Carlo
    



### Uncertainty, again

* sampling from the posterior can also be used to show the uncertainty inherent in the inference!

HPDI

### Bayesian model selection


### Bayes in machine learning


