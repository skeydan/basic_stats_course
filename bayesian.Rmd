---  
title: The Bayesian Way
output: 
  html_document
---
  
## Uncertainty

* in frequentist statistics, the true parameters are fixed, and the data is random
* in Bayesian statistics, the data is given; what's uncertain are the parameters
* in other words, parameters like means or standard deviations are modeled as random variables
* they have a _prior_ (before looking at the data) and a _posterior_ distribution (combining prior and data according to Bayes' theorem)
* the prior information and the data are combined using Bayes Theorem:

$$P(A ~|~ B) = \frac{P(B|A) * P(A)}{P(B)}$$

* the different items being
    + the prior probability of the hypothesis/parameter \(P(A)\)
    + the likelihood of the data, given the hypothesis/parameter \(P(B|A)\)
    + the evidence \(P(B)\) = the likelihood of the data, integrated over all hypotheses/parameter values
    + the posterior probability of the hypothesis/parameter, given the data \(P(A|B)\)


## Bayesian updating

* from Bayes theorem follows the idea of _Bayesian updating_
* Let's do an example.

We want to estimate the bias of a curious-looking coin we've never seen before, and we're skepticists, so we assume no value is more probable a priori than another. So, we choose a uniform distribution for our prior:

```{r, warning=FALSE, message=FALSE}
library(ggplot2)
library(dplyr)
x <- seq(0,1,0.001)
# using a beta(1,1) distribution for the prior
ggplot(data_frame(x), aes(x)) + stat_function(fun = dbeta, args = list(1,1))
```

That is, before ever seeing any data, we assume that every bias is equally likely.

Now, we throw the coin once and obtain a tail. We now update our prior assumption to take into account the data, using Bayes theorem. 
In our conveniently chosen example, we don't have to compute integrals - we make use of a concept called _conjugacy_, which means that if we choose a prior distribution that is _conjugate_ to the distribution of the likelihood, we obtain a new instance of the type of the prior distribution.
For our example of coin flips, the conjugate distributions are the beta and the binomial, and combining the beta prior with the binomial likelihood of coin flips we obtain a beta distribution again:

```{r}
x <- seq(0,1,0.001)
# having flipped the coin once and seen a tail, we now have a beta(1,12) distribution for the posterior
ggplot(data_frame(x), aes(x)) + stat_function(fun = dbeta, args = list(1,2))
```

As we see from the plot, now that we've seen one tail we can be sure of one thing: The coin cannot have two heads.

We flip again, and obtain another tail.

```{r}
x <- seq(0,1,0.001)
# tail again - posterior is now beta(1,3) 
ggplot(data_frame(x), aes(x)) + stat_function(fun = dbeta, args = list(1,3))
```

Now, we get a head, and can rule out that there's just two tails:

```{r}
x <- seq(0,1,0.001)
# tail again - posterior is now beta(1,3) 
ggplot(data_frame(x), aes(x)) + stat_function(fun = dbeta, args = list(2,3))
```

Let's throw some more, and look at how our belief develops:


```{r}
library(gridExtra)
x <- seq(0,1,0.001)
# the results obtained (first item is from the prior)
throws <- c(1,0,0,1,1,1,1,0,1,0,1,1,0,1,0,1)
successes <- cumsum(throws)
# number of throws first item is from the prior)
ns <- seq(1:16)
plots <- Map(function(s,n) {
  ggplot(data_frame(x), aes(x)) + stat_function(fun = dbeta, args = list(s,n))},
  successes,
  ns)
do.call('grid.arrange', plots)
```

As we see, our belief gets more dense in the middle part of the distribution, but we're still pretty unsure about the true bias.



## Computation and Markov Chain Monte Carlo

* Until recently, using Bayesian models was severely restricted due to computational problems:
    + if not using conjugate distributions, exactly computing the integral in the denominator may be unfeasible
    + on the other hand, a model should be chosen for adequacy, not for computational feasibility
    + solution: Markov Chain Monte Carlo
    
* Markov Chain Monte Carlo    
    + draw samples from an unknown [posterior] distribution
    + prominent algorithms: Metropolis-Hastings, Gibbs sampling, Hamiltonian Monte Carlo
    + example: Metropolis-Hastings in R
    
    
Example: King Markov
    
```{r}
# King Markov example from McElreath's Statistical Rethinking
# In the long run, each island will be visited according to its population size
num_weeks <- 1e5 # number of weeks to plan
positions <- rep(0,num_weeks) # where the King was at
# islands are ordered from 1 to 10, according to their population size
# numbers are proportions: island 2 has twice as big a population as island 1, island 3 has 3x as big a population, etc.
current <- 1 # current island
for ( i in 1:num_weeks ) {
    # record current position
    positions[i] <- current

    # flip coin to generate proposal to go left or right
    proposal <- current + sample( c(-1,1) , size=1 )
    # now make sure he loops around the archipelago
    if ( proposal < 1 ) proposal <- 10
    if ( proposal > 10 ) proposal <- 1

    # move?
    prob_move <- proposal/current
    # if proposal > current: move
    # if proposal < current: flip a coin and move or stay accordingl
    current <- ifelse( runif(1) < prob_move , proposal , current )
}
```

Where the King ended up during the first 100 days:

```{r}
ggplot(data_frame(x = 1:100, y = positions[1:100]), aes(x,y)) + geom_point()
```



## Communicating uncertainty

* Having obtained a posterior distribution for the unknown parameter, we now want to predict new data
* We do this by generating data from the posterior distribution we obtained
* this is radically different from using just the most probable posterior value for prediction!
* instead, we average over the predictions from the different parameter values, weighting every prediction by the probability of that parameter value
* this is called the _posterior predictive_ distribution
* this way, the uncertainty about the parameter is preserved and communicated in the prediction

Let's go through this. Assume that we've computed a posterior distribution over a grid of values...

```{r}
# compute the posterior on a grid of values ranging from 0 to 1
p_grid <- seq(0,1, length.out = 1000)
# uniform prior
prior <- rep(1,1000)
# assume our data is 5 successes out of 13 
# how likely is it to get 5 successes out of 13 with each of the probabilities in the grid?
likelihood <- dbinom(5,13, prob=p_grid)
posterior <- prior * likelihood
# normalized
posterior <- posterior / sum(posterior)
```

... we can now sample from the posterior. Each value of the grid is included with a probability that is its posterior:

```{r}
samples <- sample(p_grid, size = 1e4, replace = TRUE, prob = posterior)
```

This is how the samples look like:

```{r}
ggplot(data_frame(x = samples), aes(x)) + geom_density()
```

Now let's compare two kinds of predictions:

* simulating observations exclusively from the MAP (maximum aposteriori, the most probable value of the posterior distribution)
* simulating observations from the entire posterior distribution


```{r}
map <- p_grid[which.max(posterior)]
pred1 <- rbinom(1e4, size = 13, prob = map)
pred2 <- rbinom(1e4, size = 13, prob = samples)
ggplot(data_frame(pred1 = pred1, pred2 = pred2), aes(x = pred1)) + 
  geom_histogram(bins = 13, fill = 'red', color = 'red', alpha = .2) + 
  geom_histogram(aes(x = pred2), bins = 13, fill = 'yellow', alpha = .2, color = 'yellow') +
  xlab("predicted values")
```

As we see, the predictions from the entire distribution are more spread out, thus better conveying the uncertainty.


## Read on

As we don't have the time to cover more of Bayesian statistics, here are my recommendations:

* Statistical Rethinking, by Richard McElreath. Great for understanding the concepts, what really matters, and why it matters
* Doing Bayesian Data Analysis, by John Krusche. 
* Bayesian Data Analysis, by Andew Gelman. Advanced stuff.


