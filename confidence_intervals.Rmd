---
title: "Confidence intervals"
output: html_document
---

## Confidence Intervals

### From the CLT to confidence intervals

* As we saw, the sample mean of IID variables approaches a standard normal distribution:
$$\frac{\bar X_n - \mu}{\sigma / \sqrt{n}} \sim N(0,1)$$
* And we know that for a standard normal distribution,  ~ 95% of the density lies between $[\bar{x} - 2 sd, \bar{x} + 2 sd]$
* So the probability that $\bar X$ lies between $\mu + 2 \sigma / \sqrt{n}$ and $\mu - 2 \sigma / \sqrt{n}$ is ~ 95%.
* Therefore, 
$\bar X \pm 2 \sigma /\sqrt{n}$
is an approx. 95% interval for $\mu$.
* In the same way, 
$\bar X \pm 3 \sigma /\sqrt{n}$
is an approx. 99% interval for $\mu$ a.

### Confidence intervals: the mechanics

* Say we want a 90% confidence interval for the mean of some distribution
* then the upper 5% and the lower 5% of the distribution are not contained in the CI
* the upper 5% is everything _above the .95 quantile_, which is $1.645$ for a standard normal
* the lower 5% is everything _below the 0.05 quantile_, which is $-1.645$ for a standard normal
* therefore 90% of the density lies between $-1.645$ and $1.645$, in standard normal units
* converting back to the original units, we have that 90% of the density lies between
$\bar X \pm 1.645 \sigma /\sqrt{n}$


### What a confidence interval is NOT

* Constructing a 95% confidence interval around an average does __not__ mean that the true mean is contained in the interval with 95% probability!
* This kind of statement is only possible with __credible intervals__ in Bayesian methodology
* In frequentist statistics, all assertions have to be seen as assertions __in the long run__:
* 95 % confidence is a confidence that __in the long run__, 95 % of the CIs will include the population mean. It is a confidence in the algorithm and not a statement about a single CI.
* In other words: Confidence intervals are a statement about the percentage of confidence intervals that contain the true parameter value.
* Even this statement is only true __if the mean observed in the sample happened to be the true mean__! In general, in the long run, a 95% CI has an 83.4% probability of capturing the true mean (Cumming & Maillardet, 2006).

### Confidence intervals vs. prediction intervals
* Even though 95% of future confidence intervals will contain the true parameter, a 95%
confidence interval will not contain 95% of future individual observations
* For prediction of individual observations, we need __prediction intervals__
* These are always much wider than confidence intervals
* The reason is that future individual observations vary much more than future means


### Exploring the confidence interval: http://rpsychologist.com/d3/CI/


### Calculation of CIs

#### Confidence interval for a normal distribution:

```{r}
x <- rnorm(1000, mean = 100, sd = 10)
# a 95% confidence interval; qnorm(0.975) = 1.96
mean(x) + c(-1, 1) * qnorm(0.975) * sd(x)/sqrt(length(x))
# a 90% confidence interval; qnorm(0.95) = 1.64
mean(x) + c(-1, 1) * qnorm(0.95) * sd(x)/sqrt(length(x))
```


#### Confidence interval for a proportion:

* Let's imagine we've asked 100 people if they prefer apples or oranges, and 61 said they preferred oranges, yielding a preference ratio of .61. What is a 95% confidence interval for the "true proportion"?
* Preference is binary, so the sum of preferences is a binomial distribution with true mean $p$ and true variance $p(1-p)$
* theoretical CI:
$$\hat p \pm z_{1 - \alpha/2}  \sqrt{\frac{p(1 - p)}{n}}$$
* Wald CI (replacing $p$, the true parameter we don't know, by $\hat p$, the parameter estimated from the sample):
$$\hat p \pm z_{1 - \alpha/2}  \sqrt{\frac{\hat p(1 - \hat p)}{n}}$$


```{r}
p_hat <- 0.61
n <- 100
# a 95% confidence interval
p_hat + c(-1, 1) * qnorm(0.975) * sqrt((p_hat * (1-p_hat))/n)
# alternatively, use binom.test for an exact test
binom.test(61,100)$conf.int
```

