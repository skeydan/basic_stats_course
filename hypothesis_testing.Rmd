---
title: "Hypothesis Testing"
output: html_document
---

## Hypothesis testing

### Hypothesis testing: the setting

* In classical hypothesis testing, the task is to decide between two alternatives:
    + the null hypothesis $H_0$: the default we're assuming (just in order to reject it)
    + the alternative hypothesis $H_1$: the thing we really would like to show to be correct, the thing we're collecting evidence for
* We can __never__ prove that $H_0$ is false, or that $H_1$ is true. We can only hope to assemble a lot of evidence in favor of $H_1$.
* If we're successful, we say that under the assumption $H_0$ is true, the data we've collected is _(very) surprising_
* Consequently, we say we're _rejecting_ $H_0$.

### Hypothesis testing: examples

* $H_0$: The average temperature in Munich in January is 5 degrees Celsius; $H_1$: The average temperature in Munich in January is _not_ 5 degrees Celsius ($H_1$ undirected)
* $H_0$: Men and women are equally good at learning languages; $H_1$: Women are better than men at learning languages ($H_1$ directed)
* $H_0$: Marathon finisher times are independent of whether it rained during the race or not; $H_1$: There is a difference in marathon finisher times conditional on whether it was raining or not ($H_1$ undirected)

### Hypothesis testing: the logic

* How likely is the observed data, assuming $H_0$ is true?
* Example:
    + We know that in the population overall, performance on test T equals 30.
    + For a special group of 64 participants (politicians, say) we obtain an average score of 27, with a standard deviation of 6.
    + How likely is it that the politicians are not a sample of the population overall?
    
*  Using the CLT, the sample mean is approximately normally distributed, with a mean of $30$ and a standard error of $6/\sqrt{64} = 0.75$:
$$\bar X \sim N(30, 0.75)$$  
* Now, for our sample mean to be real surprising, given the distribution it is said to come from, we want it to lie in either the _very_ lower or the _very_ upper part of this hypothetical distribution. 
* Let's quickly peek at how it looks in our example:

```{r, warning=FALSE, message=FALSE}
library(ggplot2)
library(dplyr)
x <- seq(25,35, 0.1)
ggplot(data_frame(x), aes(x)) + stat_function(fun = dnorm, args = list(mean = 30, sd =0.75)) + 
  geom_vline(xintercept = 27, color = 'red') + ggtitle('Likelihood of sample mean (in red) if H0 is true')
```


* Let's say we decide that we will find the data surprising if it lies in the lower 2.5% or the upper 2.5% of the distribution. In that case, if we decide to reject $H_0$, we run a 5% risk of having made a mistake.
* This risk is the _Type 1 error_, the risk to mistakenly reject $H_0$. It is given by $\alpha$, the Type 1 error rate.
* To find the boundaries, we use the standard normal quantiles corresponding to a 5% error rate (or a 95% confidence interval, correspondingly): -1.96 and 1.96.
* So, we reject $H_0$ if either
    + $\bar X > 30 + 1.96*0.75 = 31.47$, or
    + $\bar X < 30 - 1.96*0.75 = 28.53$
* With the mean of $27$ we obtained from our sample, we thus reject $H_0$ at $alpha = 0.05$
* Had we chosen $alpha = 0.01$ instead, we would also have rejected $H_0$, because
    + $\bar X = 27 < 30 - 2.58*0.75 = 28.07$


### Hypothesis testing: commonly used Metrics

action/truth                | $\mathbf{H_0}$ is true   | $\mathbf{H_1}$ is true   | total
-------------               | -------------            | -------------            | ---
__$\mathbf{H_1}$ accepted__ | $V$                      | $S$                      | $R$
__$\mathbf{H_1}$ rejected__ | $U$                      | $T$                      | $n - R$
__total__                   | $n_0$                    | $n - n_0$                | $n$


(after: http://en.wikipedia.org/wiki/Familywise_error_rate)

* False Positive / __Type 1 error__: $V$
* False Negative / __Type 2 error__: $T$
* False Discovery Rate (__FDR__): $\frac{V}{R}$
* False Positive Rate (__FPR__): $\frac{V}{n_0}$
* Family Wise Error Rate (__FWER__): $Pr(V >= 1)$ (at least one false positive)
* Positive Predictive Value (__PPV__): 1 - FDR = $\frac{S}{R}$

&nbsp;

### Related metrics (used in e.g. retrieval, pattern recognition, data science)

* __Sensitivity__ / Recall / True Positive Rate: proportion of correctly detected positives, $\frac{S}{n - n_0}$
* __Specificity__ / True Negative Rate: proportion of negatives correctly identified as such, $\frac{U}{n_0}$
* __Precision__: proportion of accepted instances that are correct, $\frac{S}{R}$ (= Positive Predictive Value)
* __Accuracy__: proportion classified correctly, $\frac{S + U}{n}$
* see also: https://en.wikipedia.org/wiki/Precision_and_recall

&nbsp;

### Hypothesis Testing Demo
* http://shinyapps.org/apps/PPV/ 
* observe how  
    + the percentage of actually true hypotheses, 
    + the chosen alpha, and
    + the power of the test
interact in producing each of $U,V,S,T$
